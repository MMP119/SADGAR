"""
================================================================================
LOADER NORMALIZADO - M√∫ltiples Colecciones Especializadas
================================================================================
"""

import csv
import logging
from pymongo import MongoClient, ASCENDING, DESCENDING, TEXT
from tqdm import tqdm
import time
from config import (
    MONGO_URI, 
    BATCH_SIZE, 
    TSV_FILES,
    TITLE_TYPES,
    PERSON_CATEGORIES
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class IMDbNormalizedLoader:
    """
    Loader que crea m√∫ltiples colecciones especializadas
    """
    
    def __init__(self):
        self.client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
        self.db = self.client['IMDb_NoSQL']
        self.start_time = time.time()
    
    def read_tsv(self, filepath):
        """Leer archivo TSV"""
        logger.info(f"üìñ Leyendo: {filepath}")
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f, delimiter='\t')
            for row in reader:
                # Convertir \N a None
                yield {k: (None if v == '\\N' else v) for k, v in row.items()}
    
    # ========================================================================
    # PASO 1: CARGAR PEOPLE
    # ========================================================================
    
    def load_people(self):
        """Cargar personas a la colecci√≥n people"""
        logger.info("\n" + "="*80)
        logger.info("üë• PASO 1: Cargando PEOPLE")
        logger.info("="*80)
        
        self.db['people'].drop()
        
        batch = []
        count = 0
        
        for row in tqdm(self.read_tsv(TSV_FILES['people']), desc="People"):
            # Convertir profesiones
            professions = row.get('primaryProfession', '').split(',') if row.get('primaryProfession') else []
            
            doc = {
                '_id': row['nconst'],
                'nombre': row.get('primaryName'),
                'a√±o_nacimiento': int(row['birthYear']) if row.get('birthYear') and row['birthYear'].isdigit() else None,
                'a√±o_muerte': int(row['deathYear']) if row.get('deathYear') and row['deathYear'].isdigit() else None,
                'profesiones': professions,
            }
            
            batch.append(doc)
            count += 1
            
            if len(batch) >= BATCH_SIZE:
                self.db['people'].insert_many(batch, ordered=False)
                batch = []
        
        if batch:
            self.db['people'].insert_many(batch, ordered=False)
        
        logger.info(f"‚úÖ {count:,} personas cargadas")
    
    # ========================================================================
    # PASO 2: CARGAR TITLES (separado por tipo)
    # ========================================================================
    
    def load_titles(self):
        """Cargar t√≠tulos separados por tipo (movies, series, etc.)"""
        logger.info("\n" + "="*80)
        logger.info("üé¨ PASO 2: Cargando T√çTULOS (por tipo)")
        logger.info("="*80)
        
        # Drop collections
        self.db['movies'].drop()
        self.db['series'].drop()
        self.db['documentaries'].drop()
        self.db['shorts'].drop()
        self.db['episodes'].drop()
        self.db['others'].drop()
        
        batches = {
            'movies': [],
            'series': [],
            'documentaries': [],
            'shorts': [],
            'episodes': [],
            'others': []
        }
        
        counts = {k: 0 for k in batches.keys()}
        
        for row in tqdm(self.read_tsv(TSV_FILES['titles']), desc="Titles"):
            title_type = row.get('titleType')
            
            # Determinar colecci√≥n destino
            collection = None
            for col_name, types in TITLE_TYPES.items():
                if title_type in types:
                    collection = col_name
                    break
            
            if not collection:
                collection = 'others'
            
            # Convertir g√©neros
            genres = row.get('genres', '').split(',') if row.get('genres') else []
            
            # Documento base
            doc = {
                '_id': row['tconst'],
                'titulo': row.get('primaryTitle'),
                'titulo_original': row.get('originalTitle'),
                'a√±o': int(row['startYear']) if row.get('startYear') and row['startYear'].isdigit() else None,
                'generos': genres,
            }
            
            # Campos espec√≠ficos por tipo
            if collection in ['movies', 'documentaries', 'shorts']:
                doc['duracion'] = int(row['runtimeMinutes']) if row.get('runtimeMinutes') and row['runtimeMinutes'].isdigit() else None
            
            if collection == 'series':
                doc['a√±o_inicio'] = doc['a√±o']
                doc['a√±o_fin'] = int(row['endYear']) if row.get('endYear') and row['endYear'].isdigit() else None
                del doc['a√±o']
            
            batches[collection].append(doc)
            counts[collection] += 1
            
            # Insert cuando el batch est√© lleno
            if len(batches[collection]) >= BATCH_SIZE:
                self.db[collection].insert_many(batches[collection], ordered=False)
                batches[collection] = []
        
        # Insert batches restantes
        for collection, batch in batches.items():
            if batch:
                self.db[collection].insert_many(batch, ordered=False)
        
        logger.info(f"‚úÖ T√≠tulos cargados:")
        for col_name, count in counts.items():
            logger.info(f"   - {col_name}: {count:,}")
    
    # ========================================================================
    # PASO 3: AGREGAR RATINGS
    # ========================================================================
    
    def merge_ratings(self):
        """Agregar ratings a movies y series"""
        logger.info("\n" + "="*80)
        logger.info("‚≠ê PASO 3: Agregando RATINGS")
        logger.info("="*80)
        
        from pymongo import UpdateOne
        
        batch_movies = []
        batch_series = []
        
        for row in tqdm(self.read_tsv(TSV_FILES['ratings']), desc="Ratings"):
            tconst = row['tconst']
            rating = float(row['averageRating']) if row.get('averageRating') else None
            votos = int(row['numVotes']) if row.get('numVotes') else None
            
            update = UpdateOne(
                {'_id': tconst},
                {'$set': {'rating': rating, 'votos': votos}}
            )
            
            # Intentar actualizar en movies y series
            batch_movies.append(update)
            batch_series.append(update)
            
            if len(batch_movies) >= BATCH_SIZE:
                try:
                    self.db['movies'].bulk_write(batch_movies, ordered=False)
                except:
                    pass
                try:
                    self.db['series'].bulk_write(batch_series, ordered=False)
                except:
                    pass
                batch_movies = []
                batch_series = []
        
        # Restantes
        if batch_movies:
            try:
                self.db['movies'].bulk_write(batch_movies, ordered=False)
            except:
                pass
            try:
                self.db['series'].bulk_write(batch_series, ordered=False)
            except:
                pass
        
        logger.info("‚úÖ Ratings agregados")
    
    # ========================================================================
    # PASO 4: AGREGAR CREW (directores/escritores)
    # ========================================================================
    
    def merge_crew(self):
        """Agregar director_ids y writer_ids a movies/series"""
        logger.info("\n" + "="*80)
        logger.info("üé• PASO 4: Agregando CREW (directors/writers)")
        logger.info("="*80)
        
        from pymongo import UpdateOne
        
        batch_movies = []
        batch_series = []
        
        for row in tqdm(self.read_tsv(TSV_FILES['crew']), desc="Crew"):
            tconst = row['tconst']
            
            directors = row.get('directors', '').split(',') if row.get('directors') else []
            writers = row.get('writers', '').split(',') if row.get('writers') else []
            
            update = UpdateOne(
                {'_id': tconst},
                {'$set': {
                    'director_ids': directors,
                    'writer_ids': writers
                }}
            )
            
            batch_movies.append(update)
            batch_series.append(update)
            
            if len(batch_movies) >= BATCH_SIZE:
                try:
                    self.db['movies'].bulk_write(batch_movies, ordered=False)
                except:
                    pass
                try:
                    self.db['series'].bulk_write(batch_series, ordered=False)
                except:
                    pass
                batch_movies = []
                batch_series = []
        
        # Restantes
        if batch_movies:
            try:
                self.db['movies'].bulk_write(batch_movies, ordered=False)
            except:
                pass
            try:
                self.db['series'].bulk_write(batch_series, ordered=False)
            except:
                pass
        
        logger.info("‚úÖ Crew agregado")
    
    # ========================================================================
    # PASO 5: CARGAR PRINCIPALS
    # ========================================================================
    
    def load_principals(self):
        """Cargar principals (roles en producciones)"""
        logger.info("\n" + "="*80)
        logger.info("üé≠ PASO 5: Cargando PRINCIPALS")
        logger.info("="*80)
        
        self.db['principals'].drop()
        
        batch = []
        count = 0
        
        for row in tqdm(self.read_tsv(TSV_FILES['principals']), desc="Principals"):
            doc = {
                'titulo_id': row['tconst'],
                'persona_id': row['nconst'],
                'categoria': row.get('category'),
                'personaje': row.get('characters'),
                'orden': int(row['ordering']) if row.get('ordering') else None
            }
            
            batch.append(doc)
            count += 1
            
            if len(batch) >= BATCH_SIZE:
                self.db['principals'].insert_many(batch, ordered=False)
                batch = []
        
        if batch:
            self.db['principals'].insert_many(batch, ordered=False)
        
        logger.info(f"‚úÖ {count:,} principals cargados")
    
    # ========================================================================
    # PASO 6: CREAR COLECCIONES ESPECIALIZADAS DE PERSONAS
    # ========================================================================
    
    def create_specialized_people(self):
        """Crear colecciones actors, directors, writers"""
        logger.info("\n" + "="*80)
        logger.info("üë§ PASO 6: Creando colecciones especializadas de personas")
        logger.info("="*80)
        
        # Actores
        logger.info("  Creando colecci√≥n: actors")
        self.db['actors'].drop()
        
        pipeline_actors = [
            {'$match': {'profesiones': {'$in': ['actor', 'actress']}}},
            {'$project': {
                '_id': 1,
                'nombre': 1,
                'a√±o_nacimiento': 1
            }},
            {'$addFields': {'peliculas_count': 0}},
            {'$out': 'actors'}  # Escribe directo sin cargar en RAM
        ]
        
        self.db['people'].aggregate(pipeline_actors, allowDiskUse=True)
        count = self.db['actors'].count_documents({})
        logger.info(f"    ‚úÖ {count:,} actores")
        
        # Directores
        logger.info("  Creando colecci√≥n: directors")
        self.db['directors'].drop()
        
        pipeline_directors = [
            {'$match': {'profesiones': {'$in': ['director']}}},
            {'$project': {
                '_id': 1,
                'nombre': 1,
                'a√±o_nacimiento': 1
            }},
            {'$addFields': {'peliculas_count': 0}},
            {'$out': 'directors'}  # Escribe directo sin cargar en RAM
        ]
        
        self.db['people'].aggregate(pipeline_directors, allowDiskUse=True)
        count = self.db['directors'].count_documents({})
        logger.info(f"    ‚úÖ {count:,} directores")
        
        # Escritores
        logger.info("  Creando colecci√≥n: writers")
        self.db['writers'].drop()
        
        pipeline_writers = [
            {'$match': {'profesiones': {'$in': ['writer']}}},
            {'$project': {
                '_id': 1,
                'nombre': 1,
                'a√±o_nacimiento': 1
            }},
            {'$addFields': {'obras_count': 0}},
            {'$out': 'writers'}  # Escribe directo sin cargar en RAM
        ]
        
        self.db['people'].aggregate(pipeline_writers, allowDiskUse=True)
        count = self.db['writers'].count_documents({})
        logger.info(f"    ‚úÖ {count:,} escritores")
    
    # ========================================================================
    # PASO 7: CALCULAR ESTAD√çSTICAS
    # ========================================================================
    
    def calculate_statistics(self):
        """Calcular peliculas_count para actors y directors"""
        logger.info("\n" + "="*80)
        logger.info("üìä PASO 7: Calculando estad√≠sticas")
        logger.info("="*80)
        
        from pymongo import UpdateOne
        
        # Contar pel√≠culas por director
        logger.info("  Contando pel√≠culas por director...")
        pipeline = [
            {'$match': {'director_ids': {'$exists': True, '$ne': []}}},
            {'$unwind': '$director_ids'},
            {'$group': {
                '_id': '$director_ids',
                'count': {'$sum': 1}
            }}
        ]
        
        director_counts = self.db['movies'].aggregate(pipeline, allowDiskUse=True)
        updates = []
        
        for doc in tqdm(director_counts, desc="Director counts"):
            updates.append(UpdateOne(
                {'_id': doc['_id']},
                {'$set': {'peliculas_count': doc['count']}}
            ))
            
            if len(updates) >= 1000:
                try:
                    self.db['directors'].bulk_write(updates, ordered=False)
                except:
                    pass
                updates = []
        
        if updates:
            try:
                self.db['directors'].bulk_write(updates, ordered=False)
            except:
                pass
        
        # Contar pel√≠culas por actor (desde principals)
        logger.info("  Contando pel√≠culas por actor...")
        pipeline = [
            {'$match': {'categoria': {'$in': ['actor', 'actress']}}},
            {'$group': {
                '_id': '$persona_id',
                'count': {'$sum': 1}
            }}
        ]
        
        actor_counts = self.db['principals'].aggregate(pipeline, allowDiskUse=True)
        updates = []
        
        for doc in tqdm(actor_counts, desc="Actor counts"):
            updates.append(UpdateOne(
                {'_id': doc['_id']},
                {'$set': {'peliculas_count': doc['count']}}
            ))
            
            if len(updates) >= 1000:
                try:
                    self.db['actors'].bulk_write(updates, ordered=False)
                except:
                    pass
                updates = []
        
        if updates:
            try:
                self.db['actors'].bulk_write(updates, ordered=False)
            except:
                pass
        
        logger.info("‚úÖ Estad√≠sticas calculadas")
    
    # ========================================================================
    # PASO 8: CREAR √çNDICES
    # ========================================================================
    
    def create_indexes(self):
        """Crear √≠ndices optimizados"""
        logger.info("\n" + "="*80)
        logger.info("üìá PASO 8: Creando √≠ndices")
        logger.info("="*80)
        
        # People
        self.db['people'].create_index([('nombre', TEXT)])
        logger.info("  ‚úÖ people: nombre (TEXT)")
        
        # Movies
        self.db['movies'].create_index([('titulo', TEXT), ('titulo_original', TEXT)])
        self.db['movies'].create_index([('rating', DESCENDING), ('votos', DESCENDING)])
        self.db['movies'].create_index([('director_ids', ASCENDING)])
        self.db['movies'].create_index([('generos', ASCENDING)])
        self.db['movies'].create_index([('a√±o', DESCENDING)])
        logger.info("  ‚úÖ movies: 5 √≠ndices")
        
        # Series
        self.db['series'].create_index([('titulo', TEXT)])
        self.db['series'].create_index([('rating', DESCENDING), ('votos', DESCENDING)])
        logger.info("  ‚úÖ series: 2 √≠ndices")
        
        # Actors
        self.db['actors'].create_index([('nombre', TEXT)])
        self.db['actors'].create_index([('peliculas_count', DESCENDING)])
        logger.info("  ‚úÖ actors: 2 √≠ndices")
        
        # Directors
        self.db['directors'].create_index([('nombre', TEXT)])
        self.db['directors'].create_index([('peliculas_count', DESCENDING)])
        logger.info("  ‚úÖ directors: 2 √≠ndices")
        
        # Principals
        self.db['principals'].create_index([('titulo_id', ASCENDING)])
        self.db['principals'].create_index([('persona_id', ASCENDING)])
        self.db['principals'].create_index([('categoria', ASCENDING)])
        logger.info("  ‚úÖ principals: 3 √≠ndices")
        
        logger.info("‚úÖ Todos los √≠ndices creados")
    
    # ========================================================================
    # EJECUTAR TODO
    # ========================================================================
    
    def load_all(self):
        """Ejecutar carga completa"""
        try:
            self.load_people()
            self.load_titles()
            self.merge_ratings()
            self.merge_crew()
            self.load_principals()
            self.create_specialized_people()
            self.calculate_statistics()
            self.create_indexes()
            
            elapsed = (time.time() - self.start_time) / 60
            
            logger.info("\n" + "="*80)
            logger.info("‚úÖ CARGA COMPLETADA")
            logger.info(f"   Tiempo total: {elapsed:.1f} minutos")
            logger.info("="*80)
            
            # Mostrar resumen
            logger.info("\nüìä RESUMEN:")
            collections = ['people', 'movies', 'series', 'documentaries', 'shorts', 
                          'actors', 'directors', 'writers', 'principals']
            for col in collections:
                count = self.db[col].count_documents({})
                logger.info(f"   {col}: {count:,}")
            
        except Exception as e:
            logger.error(f"\n‚ùå ERROR: {e}")
            raise
        finally:
            self.client.close()


if __name__ == '__main__':
    loader = IMDbNormalizedLoader()
    loader.load_all()
